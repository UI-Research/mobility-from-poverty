---
title: "Number of membership associations per 10,000 people — City"
author: "Manuel Alcalá Kovalski"
date: now
format:
  html:
    self-contained: true
    toc: true
    css: ../../06_neighborhoods/www/web_report.css
editor_options:
  chunk_output_type: console
execute:
  warning: false
  message: false
---

```{r}
#| label: setup

library(tidyverse)
library(here)
library(sf)
library(censusapi)
library(tigris)
library(janitor)
library(tidylog)
library(naniar)
options(tigris_use_cache = TRUE)
```

## Process:

This notebook creates city-level `Number of membership associations per 10,000 people`
(one of two Social Capital Gates Mobility Metrics)

Data downloaded from: Census ZCTA Business Patterns 

1.  download social organization data from the Census API

2.  import and clean the data file (collapse to unique ZIPs)

3.  merge with the 2010 ZCTA -\> 2010 Census Place and 2020 ZCTA -\> 2021 Census Place crosswalk

4.  collapse estimates to unique Places

5.  check against official Census Place file & limit to population cutoff Places

6.  use crosswalk population data to construct the ratio (Numerator/Denominator)

7.  add data quality tag, final file cleaning, visualize metric and export to .csv file

## Download social organization data


We pull our data from `library(censusapi)`.

**Note:** This will require a [Census API key](https://api.census.gov/data/key_signup.html).
Add the key to `census_api_key-template.R` and then delete the word "template" 
from the filename. 

It is sourced below

```{r}
# label: source-census-api-key
source(here::here("06_neighborhoods", "R", "census_api_key.R"))
```

## Import and clean the CBP data file


This means:

-   fill in fips missing zeroes,

-   isolate to only the following NAICS,

-   collapse & keep only relevant variables, and

-   add the year of these data.

Below are the codes/associations included in the County Health Rankings metric.
See [here](https://www.countyhealthrankings.org/explore-health-rankings/county-health-rankings-model/health-factors/social-economic-factors/family-and-social-support/social-associations?year=2022)
for more

Codes: 813410, 713950, 713910, 713940, 711211, 813110, 813940, 813930, 813910,
and 813920.

These codes consistently denote the same categories across NAICS2012
and NAICS2017. You can check this by visitng the [NAICS page](https://www.census.gov/naics/)

**Note:** Running the chunk below can take some time, avoid re-running it to 
prevent hitting API rate limits. The file is exported and read in once again
to avoid re-running API requests.
```{r}
#| label: get-social-organization-data

fetch_cbp_data <- function(year, naics_codes_to_keep) {
  tryCatch(
    {
      Sys.sleep(2) # Add a delay to avoid rate limiting

      # Determine NAICS variable and dataset name based on the year
      naics_var <- if_else(year >= 2014 & year <= 2016, "NAICS2012", "NAICS2017")
      dataset_name <- if_else(year >= 2019, "cbp", "zbp")

      data <- getCensus(
        name = dataset_name,
        vintage = year,
        vars = c("EMP", "ESTAB", naics_var),
        region = "zipcode:*"
      ) %>%
        as_tibble() %>%
        mutate(year = year) %>%
        mutate(naics = as.numeric(!!sym(naics_var))) %>%
        filter(naics %in% naics_codes_to_keep) %>%
        select(zip_code, year, orgs = ESTAB) %>%
        drop_na(orgs) %>%
        summarize(count_orgs = sum(orgs), .by = c("year", "zip_code"))

      return(data)
    },
    error = function(e) {
      message("Error fetching data for year ", year, ": ", e$message)
      return(NULL) # Return NULL for failed requests
    }
  )
}


# Fetch and combine data
years <- c(2014:2022)
naics_codes_to_keep <- c(
  813410, 713950, 713910, 713940, 711211, 813110, 813940,
  813930, 813910, 813920
)

cbp_zip_14 <- fetch_cbp_data(years, naics_codes_to_keep)
cbp_zip <- map(years, ~ fetch_cbp_data(.x, naics_codes_to_keep)) %>%
  compact() %>% # Remove NULLs from failed requests
  list_rbind() # Combine into a single tibble


head(cbp_zip)
write_csv(cbp_zip, "06_neighborhoods/social-capital/data/cbp_zip.csv")
```


```{r}
cbp_zip <- read_csv(here("06_neighborhoods/social-capital/data/cbp_zip.csv"))
```

**Check:** Are all ZCTA codes 5 digits?

```{r}
#| label: assert-zip-code-length
stopifnot(
  all(str_length(cbp_zip$zip_code) == 5)
)
```

**Check:** Are all years present?

```{r}
#| label: test-all-years-present

# Test if all expected years are present in the resulting data
test_years <- cbp_zip %>%
  pull(year) %>% # Extract the years column
  unique() %>% # Get unique years
  sort() # Sort the years

# Check if the expected years match the test years
if (all(years %in% test_years)) {
  message("Test passed: All expected years are present in the data.")
} else {
  missing_years <- setdiff(years, test_years)
  message("Test failed: Missing years - ", paste(missing_years, collapse = ", "))
}
```

## Merge with the 2020 ZCTA -\> 2021 Census Place crosswalk

Download the 2010/2020 ZCTA -\> 2010/2021 Census Place crosswalk file from [Geocorr 2022](https://mcdc.missouri.edu/applications/geocorr2022.html) as follows:

- Ctrl-click select every state to get the whole country (where is says Select
the state(s) (including DC and/or PR) to process:)

- Then, choose ZIP/ZCTA from the 2010/2020 Geographies in the left rectangle, and
Place (city. town, etc) from the 2010/2020 Geographies in the right one

- Choose population as the weighting variable you're using to do interpolation

- In Output Options, pick CSV

- Check select for Generate second allocation factor AFACT2 showing portion of
target geocodes in source geocodes

- Hit Run Request, and it will generate the crosswalk for you!


Now, import and clean the 2010/2020 ZCTA -\> 2010/2021 Census Place crosswalk file to
prepare for merging

```{r}
#| label: import-and-clean-xwalk
zcta_20_place_21_xwalk <-
  read_csv(here("06_neighborhoods/social-capital/data/2020_ZCTA_2021_Census_Places_Crosswalk.csv"), skip = 1) %>%
  as_tibble() %>%
  clean_names() |>
  rename(
    zip_code = zip_census_tabulation_area,
    state_fips = state_code,
    place = place_code
  )

zcta_10_place_10_xwalk <-
  read_csv(here("06_neighborhoods/social-capital/data/2010_ZCTA_2010_Census_Places_Crosswalk.csv"), skip = 1) %>%
  as_tibble() %>%
  clean_names() |>
  rename(
    zip_code = zip_census_tabulation_area,
    state_fips = state_code,
    place = place_code,
    zcta_to_place_allocation_factor = zcta5_to_placefp_allocation_factor,
    place_to_zcta_allocation_factor = placefp_to_zcta5_allocation_factor
  )
```



Make an indicator for ZIPs that fall wholly into a Place vs. partially (`zcta_to_place_allocation_factor < 1`)

```{r}
#| label: make-indicator-for-zips-in-place
zcta_20_place_21_xwalk <-
  zcta_20_place_21_xwalk %>%
  mutate(zcta_fully_within_place_binary = case_when(
    zcta_to_place_allocation_factor == 1 ~ 1,
    zcta_to_place_allocation_factor < 1 ~ 0
  ))

zcta_10_place_10_xwalk <-
  zcta_10_place_10_xwalk %>%
  mutate(zcta_fully_within_place_binary = case_when(
    zcta_to_place_allocation_factor == 1 ~ 1,
    zcta_to_place_allocation_factor < 1 ~ 0
  ))
```

**Check:** Do 9,644 and 9,669 of these ZCTAs fall fully into a Census Place?

```{r}
#| label: check-portions-wholly-in-place

stopifnot(
  zcta_10_place_10_xwalk %>%
    count(zcta_fully_within_place_binary) %>%
    filter(zcta_fully_within_place_binary == 1) %>%
    pull(n) == 9644
)

stopifnot(
  zcta_20_place_21_xwalk %>%
    count(zcta_fully_within_place_binary) %>%
    filter(zcta_fully_within_place_binary == 1) %>%
    pull(n) == 9669
)
```

Now, select the relevant crosswalk variables and combine the 2010 ZCTA's to 2010
places crosswalk with the 2020 ZCTA's to 2021 Places crosswalk. 
```{r}
#| label: select-crosswalk-variables

zcta_20_place_21_xwalk <-
  zcta_20_place_21_xwalk %>%
  select(
    zip_code, state_fips, place, place_name, zcta_to_place_allocation_factor,
    place_to_zcta_allocation_factor, zcta_fully_within_place_binary
  )


zcta_10_place_10_xwalk <-
  zcta_10_place_10_xwalk %>%
  select(
    zip_code, state_fips, place, place_name, zcta_to_place_allocation_factor,
    place_to_zcta_allocation_factor, zcta_fully_within_place_binary
  )

zcta_place_xwalk <- bind_rows(zcta_20_place_21_xwalk, zcta_10_place_10_xwalk)
```

Merge the ZIP/Places crosswalk into the CBP ZIP-level data file

```{r}
#| label: merge-cbp-zip-with-crosswalk

merged_sa_zip_city <-
  cbp_zip %>%
  left_join(zcta_place_xwalk, by = c("zip_code"))
```

**Check:** Are there any missing values in the merged data for `count_orgs`?

```{r}
#| label: check-join-missingness
stopifnot(
  sum(is.na(merged_sa_zip_city$count_orgs)) == 0
)
```

## Collapse estimates to unique Places

First, as a data quality marker we create a new variable that tracks the number
of ZCTAs falling in each Place (duplicates)

```{r}
#| label: count-zip-codes-in-place

merged_sa_zip_city <-
  merged_sa_zip_city %>%
  mutate(
    count_zctas_in_place = n(),
    .by = c(year, place, place_name)
  )
```

Create the merged file where the SA numerator (`count_orgs`) is aggregated to the place level (multiply `count_orgs` by zcta to place allocation factor and then summarize to place)
and also include total ZCTAs in Place (`zip_total`) & how many of those fully
fall within the Place (`zips_in`)

```{r}
#| label: numerator-per-place-and-by-area

merged_sa_zip_city <-
  merged_sa_zip_city %>%
  # multiply `count_orgs` by the allocation factor (a measure of how much of the zcta falls within the place)
  mutate(count_orgs_afact = count_orgs * zcta_to_place_allocation_factor) %>%
  summarize(
    zip_total = mean(count_zctas_in_place),
    zips_in = sum(zcta_fully_within_place_binary),
    total_org = sum(count_orgs_afact),
    .by = c(year, state_fips, place)
  )
```

**Check:** Are there exactly 9 missing values in the `new_est_zip` variable?

```{r}
stopifnot(
  sum(is.na(merged_sa_zip_city$total_org)) == 9
)
```

We drop these missing values

```{r}
#| label: drop-missing-estimate
merged_associations_zip_city <-
  merged_associations_zip_city %>%
  drop_na(total_org)
```

## Check against Census Place file & limit to population cutoff Places

Import the updated population-cutoff Places file for the relevant years

```{r}
places_pop <-
  read_csv(here("geographic-crosswalks/data/place-populations.csv")) %>%
  rename(state_fips = state) %>%
  filter(year %in% years)

pop_2014 <- places_pop %>%
  filter(year == 2015) %>% # Take only rows for year 2015
  mutate(
    year = 2014, # Set year to 2014
    population = NA
  ) %>% # Set population to NA
  distinct(state_fips, place, state_name, place_name, year, population) # Ensure no duplicates

places_pop <- bind_rows(places_pop, pop_2014) |> arrange(year)
```

**Check:** Are there 485 places in 2015-2017 and 486 places in 2018-2022 in the population-cutoff file? 

```{r}
#| label: check-number-of-places

stopifnot(
  count(places_pop, year) %>%
    filter(year <= 2017) |>
    pull(n) %>%
    unique() == 485
)

stopifnot(
  count(places_pop, year) %>%
    filter(year > 2017) |>
    pull(n) %>%
    unique() == 486
)
```

Join `places_pop` with `merged_associations_zip_city` in order to get the final associations (numerator) city data.

**Note:** All values for 2014 will be NA since the population file is missing
2014 data.

```{r}
associations_city_data <-
  places_pop %>%
  left_join(merged_associations_zip_city,
    by = c("year", "place", "state_fips")
  )
```

## Use crosswalk population data to construct the ratio (Numerator/Denominator)

## Create the social associations per 10,000 people metric

Now, we create the social associations metric by dividing the population by
10,000 and then dividing the numerator by that value.

We also round the ratio metric to one decimal point as they do in County Health
Rankings.

```{r}
#| label: create-number-of-membership-associations-metric

associations_city_data <-
  associations_city_data %>%
  mutate(
    pop_ratio = population / 1e4,
    count_membership_associations_per_10k = round(total_org / pop_ratio, digits = 1)
  )
```

## Add data quality tag, final file cleaning and export to .csv file


## Create quality variable

Add the necessary components to the crosswalk for producing a data quality flag.
The methodology is borrowed from the [Puma to County Crosswalk notebook](geographic-crosswalks/generate_puma_county_crosswalks.qmd)

Create flags in the ZCTA-place crosswalk for places where a high percentage of the data comes from outside of the places Per agreed upon guidance, 75% or more from the place is good, below 35% is bad, in between is marginal. This is calculated by taking the product of percentage of ZCTA in the place and percentage of place in the ZCTA for each ZCTA-place pairing, and summing across the place

We then create new variable "products". This is the sum of the share of the place captured in the ZCTA and the share of the ZCTA captured in the place.

```{r}
crosswalk_data_quality <-
  zcta_place_xwalk |>
  mutate(products = place_to_zcta_allocation_factor * zcta_to_place_allocation_factor) |>
  mutate(
    sum_products = sum(products),
    .by = c("state_fips", "place")
  ) |>
  mutate(
    count_membership_associations_per_10k_quality =
      case_when(
        sum_products >= 0.75 ~ 1,
        sum_products >= 0.35 ~ 2,
        sum_products < 0.35 ~ 3
      )
  ) |>
  select(state_fips, place, count_membership_associations_per_10k_quality)

crosswalk_data_quality |>
  ggplot(aes(x = count_membership_associations_per_10k_quality)) +
  geom_histogram(color = "blue", fill = alpha("blue", 0.3), binwidth = 1.0) +
  theme_minimal() +
  ggtitle("Quality Flag 2020") +
  ylab("Count")
```


```{r}
#| label: create-data-quality-flag

associations_city_data <-
  associations_city_data %>%
  left_join(crosswalk_data_quality, by = c("state_fips", "place")) |>
  select(year, state = state_fips, place, count_membership_associations_per_10k, count_membership_associations_per_10k_quality) %>%
  arrange(
    year, state, place,
    count_membership_associations_per_10k,
    count_membership_associations_per_10k_quality
  ) %>%
  distinct()
```

**Check:** Are there any missing values in the metric?

```{r}
stopifnot(
  sum(is.na(associations_city_data %>%
    filter(year != 2014) %>%
    .[["count_membership_associations_per_10k"]])) == 0
)
```

## Visualize Metric

```{r}
ggplot(associations_city_data, aes(x = count_membership_associations_per_10k)) +
  geom_histogram(bins = 30, color = "black", fill = "lightblue") +
  labs(
    title = "Distribution of Membership Associations per 10k",
    x = "Membership Associations per 10k", y = "Frequency"
  ) +
  theme_minimal()
```

```{r}
ggplot(associations_city_data, aes(y = count_membership_associations_per_10k)) +
  geom_boxplot() +
  labs(
    title = "Boxplot of Membership Associations per 10k",
    y = "Membership Associations per 10k"
  ) +
  theme_minimal()
```

```{r}
# Visualize missing data
gg_miss_var(associations_city_data) +
  labs(title = "Missing Values by Variable")
```

```{r}
yearly_avg <- associations_city_data %>%
  group_by(year) %>%
  summarise(avg_associations = mean(count_membership_associations_per_10k, na.rm = TRUE))

ggplot(yearly_avg, aes(x = year, y = avg_associations)) +
  geom_line(linewidth = 1) +
  geom_point() +
  labs(
    title = "Trend of Average Membership Associations per 10k Over Years",
    x = "Year", y = "Average Associations per 10k"
  ) +
  theme_minimal()
```

## Run Final Tests

```{r}
source(here("functions/testing/evaluate_final_data.R"))

evaluate_final_data(here("10a_final-evaluation", "evaluation_form_social_cap1_overall_place.csv"), associations_city_data, "place", subgroups = FALSE, confidence_intervals = FALSE)
```


## Save the final file

```{r}
associations_city_data %>%
  arrange(year, state, place) %>%
  write_csv(here("06_neighborhoods/social-capital/final/social_associations_all_city.csv"))
```
