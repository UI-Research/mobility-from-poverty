---
title: "Employment Opportunities Predictor, place, non-subgroup"
author: "JP Walsh"
date: today
format:
  html:
    toc: true
    toc_float: true
    embed-resources: true
    code-fold: show
execute: 
  warning: false
editor_options: 
  chunk_output_type: console
---

*Program for the creation of the Employment Opportunities Predictor, non-subgroup, place-level*

ACS Code: Initially written by Tina Chelidze and updated by JP Walsh 2022-2023.

Primary data derived from an IPUMS extract for 2021 ACS data (1-year).

Based on processes developed by Paul Johnson and Kevin Werner in SAS.

-   [Housekeeping](#housekeeping)
-   [Create Crosswalk](#create-crosswalk)
-   [Pull microdata](#pull-microdata)
-   [Merge microdata](#merge-microdata)
-   [Create employment metric](#create-employment-metric)
-   [Export](#export)

## Housekeeping

Read in packages for use. If you do not have an IPUMS API key you will need to make one via the [IPUMS website](https://www.ipums.org/).

```{r}
library(tidyverse)
library(Hmisc)
library(ipumsr)
library(reactable)

options(scipen = 999)

#set_ipums_api_key("Your KEY", save = TRUE)
```

## Create Crosswalk

Prepare the PUMA to Place crosswalk. First read in the crosswalk file from the geographic-crosswalks folder.

```{r}

puma_place_2021 <- read_csv(here::here("geographic-crosswalks", "data", "geocorr2012_PUMA_Places_2020.csv"))

```

Rename variables for working purposes. Add leading zeros to state fips, PUMAs and places to make them uniform in length.

```{r}

puma_place_2021 <- puma_place_2021 %>%
  rename(
    puma = puma12,
    statefip = state) %>%
  mutate(
    statefip = sprintf("%0.2d", as.numeric(statefip)),
    puma = sprintf("%0.5d", as.numeric(puma)),
    place = sprintf("%0.5d", as.numeric(place))
  )


```

We want to limit the crosswalk only to Places that will be used in the final mobility metric data set. Bring in the Places population file (place-populations.csv) to select only places included in the UMF data. Keep only 2021 place population data.

```{r}
places <- read_csv(here::here("geographic-crosswalks", "data", "place-populations.csv")) %>%
  filter(year == 2021)
```

Rename the Place population variables to prep for merge.

```{r}

places <- places %>% 
  rename(statefip = state)

```

Left join crosswalk data onto the Place population file to get rid of irrelevant places data (this is in an effort to make our working files smaller).

```{r}

puma_place_2021 <- left_join(places, puma_place_2021, by=c("statefip","place"))

```

Check on the distribution of key crosswalk variables AFACT and Pop20. We want to ensure there are no abnormal outliers or missing values. 

```{r}

puma_place_2021 %>% 
  ggplot(aes(x = afact)) +
  geom_histogram(color = "blue", fill = alpha("blue", 0.3), binwidth = 0.02) +
  theme_minimal() +
  ggtitle("AFACT distribution, PUMA to Place crosswalk") +
  ylab("Count")

puma_place_2021 %>% 
  ggplot(aes(x = pop20)) +
  geom_density(color = "blue", fill = alpha("blue", 0.3)) +
  theme_minimal() +
  ggtitle("Population distribution in PUMAs, PUMA to Place crosswalk") +
  ylab("Density") 

```

Keep only the variables we will need from the crosswalk file.

```{r}

puma_place_2021 <- puma_place_2021 %>% 
  select(statefip, puma, place, pop20, afact, afact2)

```

Drop observations where the weight adjustment is zero. These PUMAs have no overlap with the places we want to include in the data.

```{r}
puma_place_2021 <- puma_place_2021 %>%
  filter(afact != 0.000)
```

### Create quality variable

Add the necessary components to the crosswalk for producing a data quality flag.

Create flags in the PUMA-place crosswalk for places where a high percentage of the data comes from outside of the Place. Per agreed upon guidance, 75% or more from the place is good, below 35% is bad, in between is marginal. This is calculated by taking the product of percentage of PUMA in place and percentage of place in PUMA for each place-PUMA pairing, and summing across the place.

Create new vars of interest. Products is the sum of the share of the place captured in the PUMA (afact2) and the share of the PUMA captured in the place (afact).

```{r}

puma_place <- puma_place_2021 %>%
  mutate(products = afact * afact2)
```

Calculate the total place population by summing pop20. Pop20 is the population in the PUMA that overlaps with the place - note that GeoCorr presents these numbers so that it already accounts for afact (see What is a Correlation List? on the GeoCorr website https://mcdc.missouri.edu/applications/docs/geocorr-help.html#weightVar).

```{r}
puma_place <- puma_place %>%
  group_by(statefip, place) %>%
  mutate(sum_products = sum(products),
         place_pop = sum(pop20)) %>% 
  ungroup() 

```

The average sum of products is 76% (more PUMAs have good quality than less than good).
We also want to know particularly small places, defined by the bottom percentile of population. The table below shows us that the first quartile of place population is 110,629.

```{r}
summary(puma_place %>% 
          select(sum_products, place_pop)) 
```

Crate an indicator of data quality based on these results. Create an indicator based on sum_product result (1 to 3 with 1 being best). Also create an indicator of small places (1 is small - define the cutoff as the first quartile calcualted above).

```{r}

puma_place <- puma_place %>%
  mutate(
    puma_flag = 
      case_when(
        sum_products >= 0.75 ~ 1,
        sum_products >= 0.35 ~ 2,
        sum_products < 0.35 ~ 3
      ),
    small_place = 
      case_when(
        place_pop >= 110629 ~ 0,
        place_pop < 110629 ~ 1
      )
  )

```

Save as "puma_place.csv" in the intermediate file.

```{r}
write_csv(puma_place, here::here("09_employment", "data", "intermediate", "puma_place.csv"))

```

Save a version with just the place-level values of data quality variables

```{r}

place_puma <- puma_place %>%
  group_by(statefip, place) %>% 
  summarise(puma_flag = mean(puma_flag), 
            small_place = mean(small_place)) %>% 
  ungroup()

```

Save as "place_puma.csv" in the intermediate file.

```{r}
write_csv(place_puma, here::here("09_employment", "data", "intermediate", "place_puma.csv"))
```

## Pull microdata

### API Pull

Using the API, read in the IPUMS micro data. To check on available surveys you can use the function get_sample_info("usa"). This version of the code is currently pulling from "2021a" which is 1-year 2021 ACS data.

Submit the extract. The directory is set to download into the "raw" data folder inside of the 09_employment folder. If the data already exists this step will be skipped.

```{r}

if(!file.exists(here::here("09_employment", "data", "raw", "employment_opp_1year.dat.gz"))){
  
    usa_ext_umf <-
      define_extract_usa(
        description = "1-year ACS microdata pull for the Employment Opportunities Predictor at the place level, non-subgroup",
        samples = c("us2021a"),
        variables = c(
          "ADJUST",
          "STATEFIP",
          "PUMA",
          "GQ",
          "AGE",
          "EMPSTAT",
          "VACANCY",
          "PERNUM"
        )
      )
    
    usa_ext_umf_submitted <- submit_extract(usa_ext_umf)
    
    usa_ext_complete <- wait_for_extract(usa_ext_umf_submitted)
    
    filepath <-
      download_extract(
        usa_ext_umf_submitted,
        download_dir = here::here("09_employment", "data", "raw"),
        progress = FALSE
      )
  }


```

Rename the IPUMS raw files. Filter for .GZ and .XML files in the raw data folder and change the names to employment_1year. Note that we filter out files that have employment in the name before renaming, this is to prevent changing downloads from the employment subgroup code. 

```{r}

if(!file.exists(here::here("09_employment", "data", "raw", "employment_opp_1year.dat.gz"))){
  
    ipums_files <-
      list.files(paste0(here::here("09_employment", "data", "raw")), full.names = TRUE) %>%
      as_tibble() %>%
      filter(str_detect(value, "dat.gz|xml"), !str_detect(value, "employment_opp")) %>%
      pull()
    
    file.rename(ipums_files, c(
      here::here("09_employment", "data", "raw", "employment_opp_1year.dat.gz"),
      here::here("09_employment", "data", "raw", "employment_opp_1year.xml")
    ))
  }
```

### Read in data

Read in the micro data extract from the raw data folder.

```{r}

ddi <-
  read_ipums_ddi(here::here("09_employment", "data", "raw", "employment_opp_1year.xml"))

micro_data <-
  read_ipums_micro(
    ddi,
    data_file = here::here("09_employment", "data", "raw", "employment_opp_1year.dat.gz")
  )


```

Reformat variable names to lowercase for easier reference throughout the program.

```{r}

acs_2021 <- micro_data %>%
  rename_with(tolower) 

```

Drop labels using zap_labels and define variable formats.
Provide leading zeros to statefip and PUMA so they match the crosswalk. 

```{r}

acs_2021 <- acs_2021 %>%
  mutate(
    across(where(is.labelled), ~ zap_labels(.x)),
    statefip = sprintf("%0.2d", as.numeric(statefip)),
    puma = sprintf("%0.5d", as.numeric(puma))
  )

```

Missing data is reported in a variety of ways by IPUMS - listed below:

Age: No missing values reported in variable description.
Empstat: N/As reported as "0".
Vacancy is reported as "0". 

There is a large share of records missing for empstat.The distribution of age looks reasonable (no bunching or extreme outliers). Note there are no vacancy results by default of the structure of census data read in.

```{r}

acs_2021 %>% 
  count(empstat) %>% 
  ggplot(mapping = aes(x = factor(empstat), y = n)) +
  geom_col() +
  theme_minimal() +
  ggtitle("Empstat, All Records") +
 geom_text(mapping = aes(label = n), vjust = -1) +    
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(x = "Employment Status",
       y = NULL)

acs_2021 %>% 
  ggplot(aes(x = age)) +
  geom_density(color = "blue", fill = alpha("blue", 0.3)) +
  theme_minimal() +
  ggtitle("Age Variable Results") +
  ylab("Density")

acs_2021 %>% 
  ggplot(aes(x = vacancy)) +
  geom_density(color = "blue", fill = alpha("blue", 0.3)) +
  theme_minimal() +
  ggtitle("Vacancy Variable Results") +
  ylab("Density")
```

Turn the missing/"not in universe" variable into NAs.

```{r}

acs2021clean <- acs_2021 %>%
  mutate(empstat = ifelse(empstat == 0, NA_integer_,
                          empstat)
  )

```

Look at distribution of empstat after adjustment. There should be no more records with the value "0".

```{r}

acs2021clean %>% 
  count(empstat) %>% 
  ggplot(mapping = aes(x = factor(empstat), y = n)) +
  geom_col() +
  theme_minimal() +
  ggtitle("Empstat, All Records") +
 geom_text(mapping = aes(label = n), vjust = -1) +    
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(x = "Employment Status",
       y = NULL)

```

Remove group quarters, we want to keep only households (GQ 1, 2 and 5).

```{r}

acs2021clean <- acs2021clean %>%
  filter(gq %in% c(1, 2, 5))

```

Check that group quarters fall only in 1, 2 and 5.

```{r}

acs2021clean %>% 
  count(gq) %>% 
  ggplot(mapping = aes(x = factor(gq), y = n)) +
  geom_col() +
  theme_minimal() +
  ggtitle("GQ, All Records") +
 geom_text(mapping = aes(label = n), vjust = -1) +    
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(x = "GQ",
       y = NULL)


```

## Merge microdata 

Join the cleaned ACS data onto the crosswalk. There will be ACS microdata records that match with multiple crosswalk records because PUMAs can intersect with more than one place. Silence the warning about this by defining the relationship as many-to-many.

```{r}

acs2021clean  <- left_join(acs2021clean, puma_place, by=c("statefip","puma"), relationship = "many-to-many")

```

The ACS data now has 3,703,719 observations. Run anti_join to see how many cases on the left did not have a match on the right

```{r}

test  <- anti_join(acs_2021, puma_place, by=c("statefip","puma"))

```

1,656,456 obs from the microdata (makes sense since we limit the records to only PUMAs that are overlapping with Places of interest).

Drop any observations with NA for afact (i.e. there is no place of interest overlapping this PUMA).

```{r}

acs2021clean <- acs2021clean %>% 
  filter(!is.na(afact))

```

3,703,719 obs to 2,131,032 obs. Also filter out cases where AFACT is equal to zero.

```{r}
acs2021clean <- acs2021clean %>% 
  filter(afact > 0)
```

Adjust weight to account for PUMA-to-county mapping (those where unique_types do not equal 1).Drop PUMA flag variable from PUMA_place.

```{r}

acs2021clean <- acs2021clean %>%
  mutate(hhwt = hhwt*afact
  ) %>% 
  select(-puma_flag)

```

Save as "microdata.csv"

```{r}
write_csv(acs2021clean, here::here("09_employment", "data", "intermediate", "2021microdata.csv"))

```

## Create employment metric

Create the employment metric (non-subgroup).

Objective: get the percent of individuals between the ages 25 and 54 that are employed from the EMPSTAT variable.

Aggregation should be weighted by PERWT (this is a person level statistic).

First, isolate the dataset to 25-54 year-olds.

```{r}
microdata_emp_age <- acs2021clean %>% 
  filter(age >= 25 & age <= 54) 
```

Check how many records, if any, have NA reported for empstat after filtering.

```{r}

microdata_emp_age %>%
  filter(is.na(empstat)) %>% 
  group_by(pernum) %>% 
  count()

```

Confirm there are no vacant properties included in the data.

```{r}

microdata_emp_age %>%
  filter(is.na(vacancy)) %>% 
  group_by(pernum) %>% 
  count()

```

Collapse # of 25-54 year olds by place
Also create a collapse var for people that age who are employed (empstat == 1).

These vars needed to calculate metric: share employed.

```{r}
metrics_employment <- microdata_emp_age %>%
  group_by(statefip, place) %>%
  summarise(
    num_in_emp_age = sum(perwt),
    num_employed = sum((empstat == 1) * perwt),
    count = n()
  )
```

Compute the ratio (share employed).

```{r}

metrics_employment <- metrics_employment %>%
  mutate(share_employed = num_employed/num_in_emp_age)

```

### Finish the Data Quality variable

For employment metric: total number of households is the sample size we are checking.

```{r}

metrics_employment <- metrics_employment %>% 
  mutate(size_flag = case_when((count < 30) ~ 1,
                               (count >= 30) ~ 0))
```

Merge the PUMA flag in & create the final data quality metric based on both size and puma flags

```{r}
metrics_employment <- left_join(metrics_employment, place_puma, by=c("statefip","place"))
```

Generate the quality var

```{r}

metrics_employment <- metrics_employment %>% 
  mutate(share_employed_quality = case_when(size_flag==0 & puma_flag==1 ~ 1,
                                     size_flag==0 & puma_flag==2 ~ 2,
                                     size_flag==0 & puma_flag==3 ~ 3,
                                     size_flag==1 ~ 3)
  )

```

Keep only relevant variables.

```{r}

metrics_employment <- metrics_employment %>%
  select(statefip,
         place,
         share_employed,
         share_employed_quality,
         size_flag,
         puma_flag)

```

### Evaluate results

Look at distribution of employment share by place.

```{r}

metrics_employment %>% 
  ungroup() %>% 
  select(place, share_employed) %>% 
  ggplot(aes(x = share_employed)) +
  geom_density(alpha = 0.15) +
  theme_minimal() +
  ggtitle("Distribution of share employed, by place") +
  ylab("Density")
  
```

Look at the counts of the quality flag.

```{r}

metrics_employment %>% 
  select(place, share_employed_quality) %>% 
  ggplot(aes(x = share_employed_quality)) +
  geom_histogram(color = "blue", fill = alpha("blue", 0.3), binwidth = 1.0) +
  theme_minimal() +
  ggtitle("Quality Flag") +
  ylab("Count")
  
```

## Export 

Rename state.

```{r}
metrics_employment <- metrics_employment %>% 
  rename("state" = "statefip")
```

Add a variable for the year of the data

```{r}

metrics_employment <- metrics_employment %>%
  mutate(
    year = 2021
  )


```

Order the variables how we want

```{r}

metrics_employment <- metrics_employment %>% 
  select(year, state, place, share_employed, share_employed_quality)

```

Export as CSV

```{r}

write_csv(metrics_employment, here::here("09_employment", "data", "final", "metrics_employment_city_2021.csv"))

```





