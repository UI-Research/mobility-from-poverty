---
title: "Agencies - Place"
author: "Vincent Pancini"
date: "`r format(Sys.time(), '%B %d, %Y %H:%M')`"
output:
  html_document:
    number_sections: false
    self_contained: TRUE
    code_folding: show
    toc: TRUE
    toc_float: TRUE
    editor_options:
      chunk_output_type: console
---

<style>
@import url('https://fonts.googleapis.com/css?family=Lato&display=swap');
</style>

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato" />


```{r rmarkdown-setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r setup}
options(scipen = 999)

library(tidyverse)
library(tidycensus)
library(skimr)
library(tidylog)
library(urbnthemes)

set_urbn_defaults(style = "print")

```

This file is a prerequisite file to the following files:

* `reported-crime-place-all.Rmd` which creates the place-level metric for reported property crimes per 100,000 people and reported violent crimes per 100,000 people
* `juvenile-arrests-place-all.Rmd` which creates the place-level metric for juvenile arrests per 100,000 juveniles

You must run this file before either of the above files to create either metric at the place-level.

This file pulls in a universe of law enforcement agencies that have reported criminal activity and arrests in 2022. It then joins on place FIPS codes to those agencies. Next, it pulls in population and demographic information from the American Community Survey (ACS). Lastly, it creates several files that are used in the creation of each metric.

The rest of this file is organized as follows:

1. Background and NIBRS Batch Header File download
2. Load and clean NIBRS Batch Header File
3. Join place FIPS codes onto agency data
4. Construct place-level population and demographics using ACS
5. Create and write out final files

All data used in this file and to create the two safety metrics are available on Box [here](https://urbanorg.box.com/s/poqnegsa6i74phcdr1eap7tacpe4x1gf).

This file assumes that you have cloned the GitHub repository for this project to your local computer. You can find the project repository [here](https://github.com/UI-Research/mobility-from-poverty) and learn how to clone a repository [here](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository).

## 1. Background and NIBRS Batch Header File download
This program and the safety metrics rely on National Incident-based Reporting System (NIBRS) data. NIBRS is an annual data collection that compiles information on criminal incidents and arrests reported by participating law enforcement agencies. It is a part of the Uniform Crime Reporting Program (UCR) which is administered by the Federal Bureau of Investigation (FBI).

NIBRS data as constructed and formatted by the FBI are stored in a single file. These data are organized by various segment levels (record types). Working with the NIBRS data in its single-file format is difficult. To facilitate use of NIBRS data, ICPSR created extract files. These files are available for download from the National Archive of Criminal Justice Data (NACJD), the criminal justice archive within ICPSR, and can be accessed here [here](https://www.icpsr.umich.edu/web/NACJD/series/128).

The segment of NIBRS data this file is concerned with is called the Batch Header File (also called the Batch Header Segment). Prior to the 2013 data, the Batch Header information was released as three segments. Due to the NIBRS data rapidly growing in size, the FBI released a single file instead. The Batch Header File identifies individual police agencies by originating agency identifier (ORI). An individual police agency (ORI) will appear in the batch header segment once. It also includes variables for each county served by each agency, which is what we are particularly interested in. The data used for both safety metrics (arrests and crimes) are reported at the agency-level. The purpose of using the Batch Header File is to be able to link the agencies to places (via counties) for those metrics.

To learn more about NIBRS data, consult Jacob Kaplan's book titled [National Incident-Based Reporting System (NIBRS) Data: A Practitioner's Guide](https://nibrsbook.com/). For these metrics, the most important thing to know about these data is that most police agencies do not report NIBRS data prior to 2021. In 2019, only 8,500 out of approximately 18,000 police agencies in the United States (covering about 45% of the US population) reported NIBRS data. Instead, agencies primarily reported [UCR data](https://ucrbook.com/) for 2020 and before. However, the FBI has moved entirely to NIBRS data starting in 2021 and no longer collects UCR data, which is why we use NIBRS data instead ([source](https://nibrsbook.com/overview-of-the-data.html)).

We do not want to use NIBRS data for some years and UCR data for others. However, because of lack of reporting, **there are a lot of missing values prior to 2021 for NIBRS data**. Future updates of the safety metrics may want to include historical NIBRS data back to 2016 and should keep this in mind.

Note that we only use the 2022 Batch Header File. Using the Batch Header File from each year might be more accurate, but for the most part it won't matter, especially because so few agencies reported through NIBRS prior to 2021. Additionally, downloading and reading in the Batch Header File for each year takes a lot of time and computing power, and must be done manually for each year which introduces room for error in replication. Future updates may consider using the Batch Header Files for each year, which can be accessed through the following links:

  * [2016](https://www.icpsr.umich.edu/web/ICPSR/studies/37066)
  * [2017](https://www.icpsr.umich.edu/web/ICPSR/studies/37650)
  * [2018](https://www.icpsr.umich.edu/web/ICPSR/studies/37649)
  * [2019](https://www.icpsr.umich.edu/web/ICPSR/studies/38565) 
  * [2020](https://www.icpsr.umich.edu/web/ICPSR/studies/38566) 
  * [2021](https://www.icpsr.umich.edu/web/ICPSR/studies/38807)
  * [2022](https://www.icpsr.umich.edu/web/ICPSR/studies/38925)

Accessing these data requires an ICPSR account. Manually downloading these data is a five-step process, but you will only need to do each of these steps once for the creation of this metric:

  1. Create an ICPSR account [here](https://www.icpsr.umich.edu/cgi-bin/newacct).
  2. Navigate to the National Archive of Criminal Justice Data (NACJD) landing page for the year you are accessing (e.g., 2022 is [here](https://www.icpsr.umich.edu/web/NACJD/studies/38925)).
  3. Select `Download`
      + Select `Delimited`
      + Select `Agree` on the `Terms of Use`
  4. Sign into your ICPSR account
  5. This will download a zip file titled `ICPSR_38925-V2`. Unzip the file and navigate to `ICPSR_38925\DS0001\38925-0001-Data.tsv`. This is the `Batch Header File`. Move this file to the `mobility-from-poverty\07_safety\data` directory after you have cloned the repository for this project from GitHub.


## 2. Load and clean NIBRS Batch Header File
This section reads in and cleans the reporting agency data contained in the Batch Header File.

### 2.1 Get universe of agencies from 2022 NIBRS Batch Header File
Load 2022 Batch Header File
```{r load-bhf, message=FALSE}
# Read in the Batch Header File
batch_header_file <- read_tsv(here::here("07_safety", "data", "38925-0001-Data.tsv"))

```

Limit file to necessary variables and rename. We use the codebook (`ICPSR_38925\DS0001\38925-0001-Codebook-ICPSR`) to determine what each variable represents.
```{r clean-bhf}
batch_header_file <- batch_header_file |>
  select(
    ori = BH003, # ori is unique agency identifier used across multiple files
    agency_type = BH012,
    state_num_nibrs = BH002,
    state_abb = BH008,
    city = BH007,
    core_city = BH013,
    fips_county_1 = BH054,
    fips_county_2 = BH055,
    fips_county_3 = BH056,
    fips_county_4 = BH057,
    # fips_county_5 = BH058, # Exists in the data but all values are missing according to codebook
  ) |>
  mutate(
    city = str_to_title(city),
    # Edit NIBRS state and county variables to have two digits and three digits respectively
    state_num_nibrs = str_pad(state_num_nibrs, 2, pad = "0"),
    county = str_pad(fips_county_1, 3, pad = "0"
    )
  ) |>
  # Drop US territories
  filter(!state_abb %in% c("GM", "PR", "VI"))

```

If an agency is only linked to one county, the other county variables have a value of `-6` for `Not Applicable`. We replace these values with `NA`. Note that in Batch Header Files prior to 2022, these observations simply have a value of `NA`.
```{r edit-bhf-na}
batch_header_file <- batch_header_file |>
  mutate(across(c(fips_county_2, fips_county_3, fips_county_4), 
                ~ ifelse(. == "-6", NA, .)))

```


### 2.2 Convert NIBRS state codes to state FIPS codes
The state identifiers that NIBRS has are state abbreviation and a state code. NIBRS state codes are in alphabetical order and do not follow FIPS codes. We manually create a crosswalk between state FIPS codes and NIBRS codes by looking at the codebook (`ICPSR_38566\DS0001\38566-0001-Codebook`) and then join state FIPS codes onto the Batch Header File.
```{r crosswalk-state-codes, message=FALSE}
# Create "crosswalk" for NIBRS state codes to state FIPS codes.
nibrs_states <- tribble(
  ~state, ~state_abb, ~state_num_nibrs, ~state_name,
  "01",	"AL", "01", "Alabama",
  "02",	"AK", "50", "Alaska",
  "04",	"AZ", "02", "Arizona", 
  "05",	"AR", "03", "Arkansas",	
  "06",	"CA", "04", "California",	
  "08",	"CO", "05", "Colorado",
  "09",	"CT", "06", "Connecticut",
  "10",	"DE", "07", "Delaware",
  "11",	"DC", "08", "District of Columbia",
  "12",	"FL", "09", "Florida",
  "13",	"GA", "10", "Georgia",	
  "15",	"HI", "51", "Hawaii",
  "16",	"ID", "11", "Idaho",
  "17",	"IL", "12", "Illinois",
  "18",	"IN", "13", "Indiana",
  "19",	"IA", "14", "Iowa",	
  "20",	"KS", "15", "Kansas",	
  "21",	"KY", "16", "Kentucky",	
  "22",	"LA", "17", "Louisiana",
  "23",	"ME", "18", "Maine",
  "24",	"MD", "19", "Maryland",
  "25",	"MA", "20", "Massachusetts",
  "26",	"MI", "21", "Michigan",	
  "27",	"MN", "22", "Minnesota",	
  "28",	"MS", "23", "Mississippi",	
  "29",	"MO", "24", "Missouri",	
  "30",	"MT", "25", "Montana",	
  "31",	"NE", "26", "Nebraska",	
  "32",	"NV", "27", "Nevada",	
  "33",	"NH", "28", "New Hampshire",	
  "34",	"NJ", "29", "New Jersey",	
  "35",	"NM", "30", "New Mexico",	
  "36",	"NY", "31", "New York",	
  "37",	"NC", "32", "North Carolina",
  "38",	"ND", "33", "North Dakota",	
  "39",	"OH", "34", "Ohio",	
  "40",	"OK", "35", "Oklahoma",	
  "41",	"OR", "36", "Oregon",	
  "42",	"PA", "37", "Pennsylvania",	
  "44",	"RI", "38", "Rhode Island",	
  "45",	"SC", "39", "South Carolina",	
  "46",	"SD", "40", "South Dakota",
  "47",	"TN", "41", "Tennessee",
  "48",	"TX", "42", "Texas",	
  "49",	"UT", "43", "Utah",	
  "50",	"VT", "44", "Vermont",	
  "51",	"VA", "45", "Virginia",	
  "53",	"WA", "46", "Washington",	
  "54",	"WV", "47", "West Virginia",	
  "55",	"WI", "48", "Wisconsin",	
  "56",	"WY", "49", "Wyoming"
)

# The NIBRS data abbreviates Nebraska as "NB" instead of its correct abbreviation "NE"
batch_header_file <- batch_header_file |>
  mutate(state_abb = if_else(state_abb=="NB", "NE", state_abb))

# Join state FIPS codes and full state name onto the NIBRS data
batch_header_file <- batch_header_file |> 
  left_join(
    y = nibrs_states, by = c("state_abb", "state_num_nibrs")
  )

```

There are 51 unique combinations of `state`/`state_name`/`state_abb`/`state_num_nibrs`, so the manual crosswalk does not seem to have any errors.
```{r check-state-codes, message=FALSE}
# Check that crosswalk worked without errors
stopifnot(
  batch_header_file %>%
    group_by(state, state_name, state_abb, state_num_nibrs) %>%
    summarize %>%
    nrow() == 51
)

```

```{r clean-environment-2.2}
# Remove obsolete objects
rm(nibrs_states)
```


### 2.3 Exclude state police and other state agencies
Check distribution of agency types. The agency types are as follows:

| Value | Label                     |
|:-----:|:--------------------------|
|   0   | Covered by another agency |
|   1   | City                      |
|   2   | County                    |
|   3   | University or college     |
|   4   | State Police              |
|   5   | Special Agency            |
|   6   | Other state agencies      |
|   7   | Tribal agencies           |
|   8   | Federal agencies          |

```{r count-agency-types}
batch_header_file |>
  count(agency_type) |>
  mutate(percent = n/nrow(batch_header_file))

```

State agencies cover multiple counties, so we remove these agencies for simplicity
```{r remove-state-agencies}
batch_header_file <- batch_header_file |>
  filter(!agency_type %in% c(4, 6))

```


## 3. Join place FIPS codes onto agency data

The goal of this section is to match a Census place FIPS code to every agency in the 2022 NIBRS Batch Header File. The Batch Header File has a variable for city name. We could just load the 486 Census places of interest from Urban and attempt to join the files by name. However, (1) this will be imprecise because of differences in naming conventions (city names in one file may have punctuation marks while city names in the other file may not), and (2) it is more complete to first assign a place GEOID to **every** agency in the data and **then** filter to agencies that align with our places of interest. We accomplish that in the following way:

1. Use LEAIC
2. Join by name using Urban’s universe of Census Places
3. Use county to place crosswalk
4. Append files

At each step, we split the data into observations that have a place FIPS code matched on and those that do not. We take those observations that still do not have a place code, try a different join, and repeat the process of splitting the observations into those that do and still do not have a place FIPS code. At the end, we append each subset to recreate the full Batch Header File.

### 3.1 Law Enforcement Agency Identifiers Crosswalk (LEAIC), 2012
Our 2022 NIBRS Batch Header File contains a list of agencies that each have a unique agency identifier (ORI). The LEAIC also contains a list of agencies with ORI, but it also contains a place FIPS code (and county FIPS) code for every agency. We merge the place FIPS code from LEAIC onto the Batch Header File by agency (ORI).

#### 3.1a Background on LEAIC and manual download
The Law Enforcement Agency Identifiers Crosswalk (LEAIC) facilitates linking reported crime data with socio-economic data. The LEAIC file is available for download from the National Archive of Criminal Justice Data (NACJD) available through ICPSR at the following link:
  * [LEAIC](https://www.icpsr.umich.edu/web/NACJD/studies/35158).
  
Note that 2012 is the most recent year for which this crosswalk is available. Future updates should check if a more recent crosswalk is available yet.
  
Accessing these data requires an ICPSR account. Manually downloading these data is a five-step process, but you will only need to do each of these steps once for the creation of this metric:

  1. Create an ICPSR account [here](https://www.icpsr.umich.edu/cgi-bin/newacct).
  2. Navigate to the National Archive of Criminal Justice Data (NACJD) landing page for the LEAIC [here](https://www.icpsr.umich.edu/web/NACJD/studies/35158)).
  3. Select `Download`
      + Select `Delimited`
      + Select `Agree` on the `Terms of Use`
  4. Sign into your ICPSR account
  5. This will download a zip file titled `ICPSR_35158-V2`. Unzip the file and navigate to `ICPSR_35158\DS0001\35158-0001-Data.tsv`. Move this file to the `mobility-from-poverty\07_safety\data` directory after you have cloned the repository for this project from GitHub.


#### 3.1b Load and clean LEAIC
After downloading the LEAIC, read it in. This file has a place FIPS linked to every reporting agency.
```{r load-leaic}
# Read in crosswalk
leaic <- read_tsv(here::here("07_safety", "data", "35158-0001-Data.tsv"))

``` 

Limit file to necessary variables and rename. We use the codebook (`ICPSR_35158\DS0001\35158-0001-Codebook`) to determine what each variable represents.
```{r clean-leaic}
leaic <- leaic |>
  mutate(
    place = str_pad(FPLACE, 5, pad = "0"),
    GEOID = str_c(FIPS_ST, place)
  ) |>
  select(
    ori = ORI9, 
    FIPS_COUNTY,
    place,
    GEOID,
    FIPS_ST,
    UANAME, # Census name for the Urbanized Area or Urban Cluster. Included for incorporated places and census-designated places only.
    LG_NAME # Local government name associated with the record. The source of these codes is the Census Bureau's Census of Governments
  ) |>
  # Remove agencies with invalid ORI
  filter(ori != "-1") # Not in UCR/NCIC

```

#### 3.1c Join LEAIC onto NIBRS data by agency identifier
Join the LEAIC to the 2022 NIBRS Batch Header File by agency.
635 agencies are in the BHF but not the 2012 crosswalk. The crosswalk is from 2012, so these could be new agencies from 2013-2022.
```{r join-bhf-leaic}
joined_ba_leaic <- left_join(
  x = batch_header_file,
  y = leaic,
  by = c("ori")
)

```

#### 3.1d Create subsets of matched and unmatched observations
From the joined Batch Header File/LEAIC, filter to the 635 observations that did not match from BHF (i.e., observations that have a missing value for the `place` variable). In the next steps, we will continue to try and assign place FIPS codes to these observations. For now, we ignore the 18,668 rows that have already matched.
```{r subset-matched-leaic}
# These are observations from the Batch Header File that still do not have a place FIPS code
ba_mis_leaic <- joined_ba_leaic |>
  filter(is.na(place)) |>
  select(ori, state_abb, state, city, county)

# Create a subset of matched observations
ba_place_leaic <- joined_ba_leaic |>
  filter(!is.na(place))
  
```


### 3.2 Join Urban Universe of Places onto NIBRS by city name
We still have 635 Batch Header File observations that do not have a place FIPS code. 276 of these observations do have a city name. We can use Urban's universe of places, which includes both city name and place FIPS codes, to match place codes onto these observations.

```{r check-city-names}
# 276 of the unmatched observations have a city name
ba_mis_leaic |>
  filter(!is.na(city)) |>
  count()

```


#### 3.2a Background on Urban’s universe of Census Places and manual download
The Urban Institute maintains a list of the 486 Census places that are of interest to this project for years 2016-2022. It is available for download on the project's GitHub repository [here](https://github.com/UI-Research/mobility-from-poverty/blob/main/geographic-crosswalks/data/place-populations.csv). For this step, you will need to download the file and move it to your appropriate working directory. 

#### 3.2b Load and clean places universe
After downloading the places file, read it in. This file contains our 486 places of interest with both their name and place FIPS code.
```{r load-urban-places, message=FALSE}
# Read in Urban places file
places <- read_csv(file = here::here("07_safety", "data", "place-populations.csv")) |>
  # Note that future updates of the safety metrics may want to include historical NIBRS data back to 2016, and in that case should not filter the places file by year
  filter(year %in% c(2021, 2022))

places |>
  group_by(year) |>
  count()

```

We are only using the 2022 Batch Header File, so we can filter to just the 2022 places. We will use the same linkage for all years. Additionally, we clean some of the variables to make the process of joining easier.
```{r clean-urban-places, message=FALSE}
places <- places |>
  # Filter year
  filter(year==2022) |>
  # Clean variables for joining
  mutate(
    # place_name = str_to_title(place_name),
    GEOID = str_c(state, place)
  )

```

Observations in the Urban places file have `city`, `village`, `town`, etc. attached to the values in the `place_name` variable, while observations in the Batch Header File do not. We create a new variable without these endings in the Urban places file to increase the chances of joining by name.
```{r clean-urban-places-names}
# Create variable for joining
places$city <- sub(" city$| municipality$| village$| town$", "", places$place_name)

```

Lastly, I am manually editing the names of a few observations from the Urban file that were flagged in the 2021 version of this metric.
```{r manually-edit-places}
places <- places |>
  mutate(city = case_when(
    state == "06" & city == "San Buenaventura (Ventura)" ~ "Ventura",
    state == "12" & city == "Fort Lauderdale" ~ "Ft Lauderdale",
    state == "13" & city == "Augusta-Richmond County consolidated government (balance)" ~ "Augusta",
    state == "18" & city == "Indianapolis city (balance)" ~ "Indianapolis",
    state == "29" & city == "St. Louis" ~ "Saint Louis",
    state == "37" & city == "Winston-Salem" ~ "Winston Salem",
    TRUE ~ city
  )
  )

```


#### 3.2c Join
Start with the 635 Batch Header File observations that still do not have a place FIPS code. Join the universe of Urban places onto these observations using state FIPS code and city name
```{r join-bhf-urban}
joined_ba_urb <- left_join(
  x = ba_mis_leaic,
  y = places,
  by = c("state", "city")
)

# Create a subset of observations that now have a place FIPS code
ba_place_urb <- joined_ba_urb |>
  filter(!is.na(place))

# Create a subset of observations that still do not have a place FIPS code
ba_mis_urb <- joined_ba_urb |>
  filter(is.na(place)) |>
  select(ori, state_abb, city, state, county)

```


### 3.3 Join county-place crosswalk onto NIBRS by county FIPS
We still have 496 Batch Header File observations that do not have a place FIPS code. 469 of these observations do have a county FIPS code. We can use a crosswalk of counties to places, which includes both county FIPS codes and place FIPS codes, to match place codes onto these observations.

#### 3.3a Background on county-place crosswalk
The Urban Institute maintains a county-place crosswalk for 2022 counties. It is available for download on the project's GitHub repository [here](https://github.com/UI-Research/mobility-from-poverty/blob/main/geographic-crosswalks/data/geocorr2022_county_place.csv). It was constructed using Geocorr 2022 from the Missouri Census Data Center, which can be accessed [here](https://mcdc.missouri.edu/applications/geocorr2022.html). For this step, you will need to download the file and move it to your appropriate working directory. 

Census places and counties are both nested within states; that is, places and counties do not share boundaries [do overlap](https://www2.census.gov/geo/pdfs/reference/geodiagram.pdf), and they do overlap. This crosswalk file contains one observation for every unique county/place pair in 2022, along with place FIPS codes and county FIPS codes, and includes observations for county components that do not overlap with a place.

We want to use this crosswalk to merge a single place code onto each remaining agency in the NIBRS data by county. This is complicated by the fact that some counties overlap with multiple places. Consider the following example: Suppose that Wayne County, MI (county FIPS 26163) has observations in the crosswalk for 4 places (Detroit, Dearborn, Livonia, and Westland). Suppose that Agency A in the NIBRS data also has a county FIPS of 26163. If we try to merge place codes from the crosswalk onto the NIBRS data by county FIPS, it won't be a 1 to 1 match, and we don't actually know which of those 4 places to associate with Agency A. However, if there is a county in the crosswalk with only one place associated with it, that is a 1 to 1 match and we can merge the place FIPS code from the crosswalk onto the NIBRS data by county FIPS code.


#### 3.3b Load and clean county-place crosswalk
After downloading the county-place crosswalk, read it in.
```{r load-county-place-xw}
county_place_xw <- read_csv(here::here("07_safety", "data", "geocorr2022_county_place.csv"))

```

Filter the crosswalk observations to only places in our Urban universe of places. 
```{r filter-xw, message=FALSE}
# First create a state/place GEOID
county_place_xw <- county_place_xw |>
  mutate(
    state = as.character(state),
    state = str_pad(state, 2, pad = "0"),
    place = as.character(place),
    place = str_pad(place, 5, pad = "0"),
    GEOID = str_c(state, place)
  )

# Filter crosswalk
county_place_xw <- county_place_xw |>
  filter(GEOID %in% places$GEOID)

```

Filter the crosswalk observations only those counties that correspond with one place.
```{r keep-1to1-matches}
county_place_xw <- county_place_xw |>
  group_by(state, county) |>
  mutate(n = n()) |>
  ungroup() |>
  filter(n == 1)

```


#### 3.3c Join county-place crosswalk onto NIBRS by county FIPS
```{r join-bhf-cpxw}
joined_ba_cpxw <- left_join(
  x = ba_mis_urb,
  y = county_place_xw,
  by = c("state", "county")
)

# Create a subset of observations that now have a place FIPS code
ba_place_cpxw <- joined_ba_cpxw |>
  filter(!is.na(place))

# Create a subset of observations that still do not have a place FIPS code
ba_mis_cpxw <- joined_ba_cpxw |>
  filter(is.na(place))
  # select(ori, state_abb, city, state, county)


```


### 3.4 Append all subsets of the NIBRS Batch Header File back together
```{r append-all-bhf}
bhf_place <- bind_rows(
  ba_place_leaic,
  ba_place_urb,
  ba_place_cpxw,
  ba_mis_cpxw
)

rm(ba_place_leaic, ba_place_urb, ba_place_cpxw, ba_mis_leaic, ba_mis_urb, ba_mis_cpxw)

```

407 agencies from the Batch Header File still do not have a place FIPS code. These could be agencies that were not in the LEAIC crosswalk, originally had no city or county information, counties that were not associated with our universe of places or that were associated with more than one place in our universe, etc.
```{r check-still-missing-place}
bhf_place |>
  filter(is.na(place)) |>
  count()

```

Finally, now that we have assigned a place FIPS code to as many agencies in the batch header file as possible, we limit to the places in Urban's universe of places
```{r limit-places}
bhf_place_urb <- bhf_place |>
  filter(GEOID %in% places$GEOID)

```


## 4. Construct place-level population and demographics using ACS
Recall that this file serves as a precursor file to the files that create both of our metrics, which are:

* Reported violent crimes per 100,000 people and reported property crimes per 100,000 people
* Juvenile arrests per 100,000 juveniles

Additionally, for the juvenile arrests metric, we are interested in subgroups by age, race, and sex. To calculate these rates (both total and by subgroup), we need total population and demographic information for our denominators. We pull total population and demographic data from the 5-year ACS files for each year 2021-2022.

This section relies on the `tidycensus` package, which requires a Census API key. You can acquire a key [here](https://api.census.gov/data/key_signup.html) and learn more about installing your key [here](https://walker-data.com/tidycensus/reference/census_api_key.html). Replace `[YOUR-KEY-HERE]` in the code below with your Census API key (leave the quotation marks).
```{r set-api-key}
# set Census API once
# tidycensus::census_api_key("[YOUR-KEY-HERE]", install=TRUE, overwrite = TRUE)

```

### 4.1 Check ACS variable names and identify those we need
Load ACS variables for our first and last years. Manually explore each file and spot check several observations. The naming conventions of ACS variables do not seem to change during our time period.
Note that if they did we would need to split up the code that reads in the years below.
Note that future updates of the safety metrics may want to pull data back to 2016, in which case the first year should be changed below.
```{r check-acs-variables}
# Check ACS variables for the first and last years to see if they change over time
variables_fy <- tidycensus::load_variables(2021, "acs5")
variables_ly <- tidycensus::load_variables(2022, "acs5")

```

### 4.2 Create vectors for years and variables of interest
Note that if additional subgroups for either safety metric are desired in future updates, the variables pulled here may need updated.
Note that future updates of the safety metrics may want to pull data back to 2016, in which case the list of years should be changed below.
```{r specify-acs-years-vars}
# Create a list of all our years
# years <- lst(2016, 2017, 2018, 2019, 2020, 2021, 2022)
years <- lst(2021, 2022)

# Create a vector for our ACS variables of interest
my_vars <- c(
  total_people = "B01003_001",
  age_m_1014 = "B01001_005", 
  age_m_1517 = "B01001_006",
  age_f_1014 = "B01001_029", 
  age_f_1517 = "B01001_030",
  age_m_1014_white = "B01001A_005", 
  age_m_1014_black = "B01001B_005",
  age_m_1014_aian = "B01001C_005", 
  age_m_1014_asin = "B01001D_005",
  age_m_1014_nhpi = "B01001E_005", 
  age_m_1014_othr = "B01001F_005",
  age_m_1014_twom = "B01001G_005",
  age_m_1014_white_nh = "B01001H_005",
  age_m_1014_hispanic = "B01001I_005",
  age_m_1517_white = "B01001A_006", 
  age_m_1517_black = "B01001B_006",
  age_m_1517_aian = "B01001C_006", 
  age_m_1517_asin = "B01001D_006",
  age_m_1517_nhpi = "B01001E_006", 
  age_m_1517_othr = "B01001F_006",
  age_m_1517_twom = "B01001G_006",
  age_m_1517_white_nh = "B01001H_006", 
  age_m_1517_hispanic = "B01001I_006",
  age_f_1014_white = "B01001A_020", 
  age_f_1014_black = "B01001B_020",
  age_f_1014_aian = "B01001C_020", 
  age_f_1014_asin = "B01001D_020",
  age_f_1014_nhpi = "B01001E_020", 
  age_f_1014_othr = "B01001F_020",
  age_f_1014_twom = "B01001G_020",
  age_f_1014_white_nh = "B01001H_020",
  age_f_1014_hispanic = "B01001I_020",
  age_f_1517_white = "B01001A_021", 
  age_f_1517_black = "B01001B_021",
  age_f_1517_aian = "B01001C_021", 
  age_f_1517_asin = "B01001D_021",
  age_f_1517_nhpi = "B01001E_021",
  age_f_1517_othr = "B01001F_021",
  age_f_1517_twom = "B01001G_021",
  age_f_1517_white_nh = "B01001H_021", 
  age_f_1517_hispanic = "B01001I_021"
)

```

### 4.3 Pull 5-year ACS data for all years 2021-2022 at the place level 
Pull 5-year ACS data at the place level for each year
```{r load-acs-data}
# Pull data
place_demo_full <- map_dfr(
  years,
  ~ tidycensus::get_acs(
    geography = "place",
    variables = my_vars,
    year = .x,
    survey = "acs5",
    output = "wide",
    geometry = FALSE
  ),
  # Create a year variable for each year
  .id = "year"
)

# Remove Puerto Rico from ACS data
place_demo <- place_demo_full |>
  mutate(state = substr(GEOID, 1, 2)) |>
  filter(state != "72")

# Remove obsolete files
rm(variables_fy, variables_ly, years, my_vars)

```

### 4.4 Clean ACS data
Clean variable names and drop margins of error
```{r clean-acs-data}
place_demo <- place_demo |>
  rename_with(~ sub("E$", "", .x), everything()) |>
  select(-c(ends_with("M")))

```

#### 4.4a Construct subgroups for race/ethnicity, sex, and age subgroups
For the rate of juvenile arrests per 100,000 juveniles metric, we want subgroups by race, sex, and age subgroups. These are subgroups of all observations ages 10-17, so the sum of each subgroup should equal the total number of observations ages 10-17.

  * Race: white, Black, Hispanic, Asian/other
  * Sex: male, female
  * Age: ages 10-14, ages 15-17
    
Note that there is a Non-Hispanic white category, but not a Non-Hispanic category for other races (Black, Asian, etc.). This means that the categories will have a small but non-zero overlap. The race/ethnicity categories actually used in the `juvenile-arrests` metric are white, Black, Asian/other, and Hispanic. These are not mutually exclusive (e.g., someone could be counted as both Black and Hispanic), which matches the population denominators created here (this is because ACS doesn't have counts of non-Hispanic Black or other non-Hispanic races other than white).
```{r create-subgroups}
# Create the total for ages 10-17 and the race, sex, and age subgroups
place_demo <- place_demo |>
  mutate(
    # Total age 10-17
    age_1017 = age_m_1014 + age_m_1517 + age_f_1014 + age_f_1517,
    # Race subgroups
    age_1017_white = age_m_1014_white + age_m_1517_white + age_f_1014_white + age_f_1517_white,
    age_1017_black = age_m_1014_black + age_m_1517_black + age_f_1014_black + age_f_1517_black,
    age_1017_aian = age_m_1014_aian + age_m_1517_aian + age_f_1014_aian + age_f_1517_aian,
    age_1017_asin = age_m_1014_asin + age_m_1517_asin + age_f_1014_asin + age_f_1517_asin,
    age_1017_nhpi = age_m_1014_nhpi + age_m_1517_nhpi + age_f_1014_nhpi + age_f_1517_nhpi,
    age_1017_othr = age_m_1014_othr + age_m_1517_othr + age_f_1014_othr + age_f_1517_othr,
    # age_1017_twom = age_m_1014_twom + age_m_1517_twom + age_f_1014_twom + age_f_1517_twom,
    age_1017_hispanic = age_m_1014_hispanic + age_m_1517_hispanic + age_f_1014_hispanic + age_f_1517_hispanic,
    age_1017_white_nh = age_m_1014_white_nh + age_m_1517_white_nh + age_f_1014_white_nh + age_f_1517_white_nh,
    # Sex subgroups
    sex_1017_m = age_m_1014 + age_m_1517,
    sex_1017_f = age_f_1014 + age_f_1517,
    # Age subgroups
    age_1014_all = age_m_1014 + age_f_1014,
    age_1517_all = age_m_1517 + age_f_1517
  )

# Edit race variables
place_demo <- place_demo |>  
  mutate(
    # Manually create "two or more" race variable
    age_1017_twom = age_1017 - age_1017_white - age_1017_black - 
      age_1017_aian - age_1017_asin - age_1017_nhpi - age_1017_othr,
    # Combine all asian races, other races, and two or more races
    age_1017_asian_other = age_1017_aian + age_1017_asin + age_1017_nhpi +
      age_1017_othr + age_1017_twom
  )

```

Limit ACS data to relevant variables 
```{r limit-acs-vars}
# Limit to relevant variables
place_demo <- place_demo |>
  select(year, GEOID, total_people, starts_with("age_1017"), starts_with("sex_1017"), ends_with("_all"))

```

Limit ACS data to places in the Urban universe of 486 places
```{r limit-to-urban-places}
# Limit places to those in the Urban file
place_demo <- place_demo |>
  filter(GEOID %in% places$GEOID)

```

Test that construction of each subgroup worked as expected
```{r test-race}
stopifnot(
  rowSums(select(place_demo, matches("age_1017_asian_other|age_1017_white$|age_1017_black")), na.rm = T) == place_demo$age_1017
)

```

```{r test-age}
stopifnot(
  rowSums(select(place_demo, matches("age_1517_all|age_1014_all")), na.rm = T) == place_demo$age_1017
)

```

```{r test-sex}
stopifnot(
  rowSums(select(place_demo, matches("sex_1017_m|sex_1017_f")), na.rm = T) == place_demo$age_1017
)

```



## 5. Create and write out final files

### 5.1 Place/year-level population and demographics
This file contains Urban's universe of 486 places by year with population and demographic information.

We've already created it, so we just need to write it out.
```{r write-place-file}
# This file has 486 places by 2 years = 972 observations
write_csv(place_demo, file = "modified data/all_place_demo.csv")

```

### 5.2 Place/year-level population and demographics with number of reporting agencies
This file contains Urban's universe of 486 places by year with population and demographic information, and includes the total number of reporting agencies in each place

First, count the number of agencies per place in our universe of places.
470 places have any agency. This is consistent with the number of places that had any agency in the 2021 safety metrics update. The maximum number of agencies in a place is 37.
```{r create-place-agency-demo}
place_agency <- bhf_place_urb |>
  # filter(!is.na(place)) |>
  group_by(state, place) |>
  summarize(
    n_agencies = n_distinct(ori),
    n_core_city = sum(core_city==1),
    core_city = max(core_city),
    n_agen_city = sum(agency_type==1),
    n_agen_cnty = sum(agency_type==2),
    n_agen_univ = sum(agency_type==3),
    n_agen_spcl = sum(agency_type==5),
    n_agen_trbl = sum(agency_type==7)
  ) |>
  ungroup() |>
  mutate(GEOID = str_c(state, place))
  
skim(place_agency)

```

Join the place-level number of agencies onto the place/year-level population and demographic data
```{r create-place-agency-demo-2}
# This file has 486 places by 2 years = 972 observations. Only 470/486 places had any agency, so 32 observations (accounting for all years) are only in the demographic file
place_demo_agency <- left_join(
  x = place_demo,
  y = place_agency,
  by = c("GEOID")
)

```

Note that `state` is missing for some observations that are missing agencies but the full state/place `GEOID` is not
```{r check-place-agency-demo-geos}
place_demo_agency |>
  filter(is.na(n_agencies)) |>
  group_by(GEOID) |>
  count()

```

Write out the file
```{r write-place-agency-demo}
write_csv(place_demo_agency, file = "modified data/all_place_demo_agency.csv")

```

### 5.3 Unique place/agency combinations
This file contains each unique place/agency pair using Urban's universe of 486 places. There are 1,145 unique observations.

We've already created this file. We limit to the variables we want to keep and then write it out.
```{r select-vars-place-agency}
# Select variables
bhf_place_urb <- bhf_place_urb |>
  select(ori, state, place, core_city, agency_type, GEOID)

```

We still want agencies that aren't reporting to have a year attached, so we add a year to each agency/place pair. 
Note that I think this is necessary (at least in part) because we are only using the Batch Header File from 2022. If we used the Batch Header File from each year, I don't think this step would be necessary.
```{r create-long-by-year}
bhf_place_urb <- bhf_place_urb |>
  expand_grid(year = c(2021, 2022))

```

```{r write-place-agency}
write_csv(bhf_place_urb, file = "modified data/all_agency_place.csv")

```

